# CADE: Detecting and Explaining Concept Drift Samples  for Security Applications
## 背景和需要解决的问题
由于网络流量的是不停的变化的，所以对于一个基于机器学习或者深度学习的流量监测系统来说，这就意味着，他每时每刻都需要接收新的数据（可能是好的，也可能不好的也就是带有攻击性的），并进行训练。这就带来了一个问题，随着数据量越来越多，一开始分类的流量可能会出现概念漂移的现象。  
为了理解什么是概念漂移，这里举一个简单的例子，假设在一个用于恶意流量监测的深度学习模型中，模型的训练数据是已知的恶意软件样本和正常软件样本，那么在真实的网络环境中，流量是每时每刻都进入模型进行监测的，在这个过程中，如果出现了一个全新的恶意软件，并且使用了不同于原先数据集中的代码结构和特征，甚至他在流量中的分布也不同，也就是这个恶意流量的数据在原数据集中并不存在，那么模型就会出现误判，也就是将其判断为安全的软件，也就是漏报，这就是概念漂移简单的理解。  
所以，对于传统的机器学习和深度学习来说，如果想要保证监测的准确率，就需要每隔一段时间对模型进行重新训练，但是这种行为是非常耗费资源的，因此，这篇论文提出了一种叫做CADE的方法。
![CADE的高层次工作流程](https://github.com/makabal/paper/blob/main/tupian/CADE-1.jpg)  
这篇文章将一个模型对新的数据的监测分成了两个空间，一个叫做Production Space，也就是模型工作的空间，一个叫做Monitoring Space也就是CADE系统工作的空间，每当新的数据进来的时候，CADE模型就会分析这个数据是否产生了数据漂移，并对其进行解释，也就是为什么会产生漂移的原因，然后告诉模型，这个应该分成一个什么类别（暂时还不知道他用的什么方法）。 
总的来说，CADE是一种对抗概念漂移的方法。  
## CADE的实现  
[源码地址](https://github.com/whyisyoung/CADE)  
### 漂移样本的监测  
主要思想：并不是单纯的数据分类，而是通过对比学习的思想，将数据的相似度进行量化的比较，如果相似度高，则作为概念漂移数据的候选。  
在这个模块中主要解决了这几个问题：  
- 如何将数据进行降维，同时又保留他的关键信息？
- 如何衡量所谓的相似度？  
#### 如何将数据进行降维，同时又保留他的关键信息？  
文章采用了autoencoder，不过这个autoencoder的特殊之处在于，它增强了对比损失，所以在这篇文章中也叫做Contrastive Auto-encoder，以下是其损失函数的构造公式：  








